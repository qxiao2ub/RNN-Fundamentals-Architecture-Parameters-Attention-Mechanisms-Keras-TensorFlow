ğŸ“˜ RNN Basics: Architecture & Attention
ğŸ“Œ Overview

This Jupyter Notebook explores fundamental concepts of Recurrent Neural Networks (RNNs), including parameter calculation, architectural behavior, sequence modeling properties, and the role of attention mechanisms.

The notebook combines theoretical reasoning with practical Keras/TensorFlow implementation insights, making it suitable for both academic study and foundational deep learning review.

ğŸ§  Topics Covered

SimpleRNN architecture and parameter computation

Many-to-one vs many-to-many sequence mappings

Effects of reversed sequences in RNN training

Key limitations of vanilla RNNs

Vanishing gradient problem

Encoderâ€“decoder bottleneck issue

Introduction to attention mechanisms

Conceptual comparison: RNN vs LSTM vs Attention

ğŸ› ï¸ Technologies Used

Python

TensorFlow / Keras

Jupyter Notebook

ğŸ¯ Learning Objectives

After completing this notebook, readers should be able to:

Compute the number of parameters in a SimpleRNN layer

Explain structural limitations of vanilla RNNs

Identify major training challenges (e.g., vanishing gradients)

Understand why attention improves sequence modeling performance

Conceptually compare RNN-based encoderâ€“decoder models with attention-based models
